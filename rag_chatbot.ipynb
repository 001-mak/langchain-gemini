{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain\n",
        "!pip install -U langchain-community\n",
        "!pip install -U langchain-google-genai\n",
        "!pip install -U chromadb\n",
        "!pip install -U google-generativeai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mFYPTxwGiKek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Access the Gemini API key from Colab secrets\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "SSnHGxQzwcNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import shutil\n",
        "import os"
      ],
      "metadata": {
        "id": "WuG8FCw-w2vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "gqbuCYVFDoIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "# Path to your PDF file\n",
        "DATA_PATH = \"/content/data.pdf\"\n",
        "\n",
        "# Path for Chroma database\n",
        "CHROMA_PATH = \"/content/chroma_db\"\n",
        "\n",
        "def main():\n",
        "    generate_data_store()\n",
        "\n",
        "def generate_data_store():\n",
        "    documents = load_documents()\n",
        "    chunks = split_text(documents)\n",
        "    save_to_chroma(chunks)\n",
        "\n",
        "def load_documents():\n",
        "    loader = PyPDFLoader(DATA_PATH)\n",
        "    documents = loader.load()\n",
        "    return documents\n",
        "\n",
        "def split_text(documents: list[Document]):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "    document = chunks[10]\n",
        "    print(document.page_content)\n",
        "    print(document.metadata)\n",
        "    return chunks\n",
        "\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "    # Clear out the database first.\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "    # Create a new DB from the documents.\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\" , google_api_key=GOOGLE_API_KEY)\n",
        "    db = Chroma.from_documents(\n",
        "        chunks, embeddings, persist_directory=CHROMA_PATH\n",
        "    )\n",
        "    db.persist()\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moMzekYTALAE",
        "outputId": "967520b4-03d4-4d3b-b991-30cb932fd3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 222 documents into 2300 chunks.\n",
            "Preamble  ...........................................................................................................................  1 \n",
            "PART I .................................................................................................................... 3\n",
            "{'source': '/content/data.pdf', 'page': 2, 'start_index': 258}\n",
            "Saved 2300 chunks to /content/chroma_db.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query template\n",
        "\n",
        "# Load the persisted database\n",
        "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY))\n",
        "\n",
        "# Perform a similarity search\n",
        "query = \"Who is the supreme power?\"\n",
        "results = db.similarity_search(query, k=3)  # k is the number of results to return\n",
        "\n",
        "# Print the results\n",
        "for doc in results:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "rLamdhzRGVwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Load the persisted database\n",
        "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY))\n",
        "\n",
        "# Create a retriever\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "# Create a ChatGoogleGenerativeAI instance\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Create a RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "\n",
        "# Ask a question\n",
        "question = \"puishment of murder?\"\n",
        "response = qa_chain.run(question)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP_lrqV-Iq2X",
        "outputId": "ca93ce55-a24c-4bd4-e8bf-afbd2f061713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text focuses on legal protections and doesn't explicitly mention the punishment for murder. It covers topics like compulsory service, retrospective punishment, double punishment, and self-incrimination. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}